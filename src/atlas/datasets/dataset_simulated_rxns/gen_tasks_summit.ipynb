{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from seaborn) (1.6.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from seaborn) (3.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from seaborn) (1.1.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/riley/Software/anaconda3/envs/olympus/lib/python3.7/site-packages/python_dateutil-2.8.1-py3.7.egg (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: six in /home/riley/Software/anaconda3/envs/olympus/lib/python3.7/site-packages/six-1.15.0-py3.7.egg (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/riley/Software/anaconda3/envs/torch/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2020.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/riley/Software/anaconda3/envs/torch/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "from summit.benchmarks import  (\n",
    "        MIT_case1, MIT_case2, MIT_case3,\n",
    "        MIT_case4, MIT_case5,\n",
    ")\n",
    "from summit.strategies import LHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinetic model\n",
    "\n",
    "For a general bi-molecular reaction $A+B \\rightarrow R$ catalyzed by a transition metal complex with concentration $C_{\\text{cat}}$, the objective function can be defined as follows assuming a closed system with a constant reaction volume. The product yield is calculated based on the initial concentration of $A$, $C_{A_o}$,\n",
    "\n",
    "$$ \\phi (\\mathbf{x}, \\mathbf{y}) = \\log(\\text{TON}) = \\log \\left( \\frac{C_R}{C_{\\text{cat}}} \\right)$$\n",
    "\n",
    "$$ Y (\\mathbf{x}, \\mathbf{y}) = \\frac{C_R}{C_{A_0}}  $$\n",
    "\n",
    "In the original paper (DOI: 10.1039/c8re00032h), the authors select the economic use of catalyst, i.e. maximization of the TON as the primary objective, subject to the yield being greater than some threshold value. \n",
    "\n",
    "The full form of the equations are as follows\n",
    "\n",
    "$$ \\log(\\text{TON}) = \\log \\left( \\frac{C_R}{C_{\\text{cat}}} \\right) \\alpha \\log(A_i) - \\frac{E_{A_i}}{R} \\frac{1}{T} - \\frac{E_{A_R}}{R} \\frac{1}{T} + (r-1) \\log(C_{\\text{cat}}) + \\log(t_{\\text{res}})\n",
    "$$\n",
    "\n",
    "$$ \\log(Y) = \\log \\left( \\frac{C_R}{C_{A_0}} \\right) \\alpha \\log(A_i) - \\frac{E_{A_i}}{R} \\frac{1}{T} - \\frac{E_{A_R}}{R} \\frac{1}{T} + r \\log(C_{\\text{cat}}) + \\log(t_{\\text{res}})\n",
    "$$\n",
    "\n",
    "\n",
    "The authors study five specififc cases \n",
    "\n",
    "\n",
    "|  Case   | Catalyst effect  |    $k_{S_1}$  |  $k_{S_2}$ |\n",
    "|:--------|:-----------------|:--------------|:-----------|\n",
    "|One optimum: 1 | $E_{A_1} > E_{A_{2-8}}$ | $=0$ | $=0$ | \n",
    "\n",
    "\n",
    "\n",
    "The optimization consists of 1 categorical/discrete parameter and 3 continuous process parameters\n",
    "\n",
    "* Catalyst ('1'-'8')\n",
    "* Temperature, T (30 - 110 degC)\n",
    "* Reaction time, $t_{\\text{res}}$ (1 - 10 min)\n",
    "* Catalyst concentration, $C_{\\text{cat}}$ (0.835-4.175mM or  0.5-2.5 mol%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert summit params to mnemosyne \n",
    "\n",
    "def convert_params(sub_df):\n",
    "    conc_cat = sub_df['conc_cat'].values.reshape(-1,1)\n",
    "    time = sub_df['t'].values.reshape(-1, 1)\n",
    "    cat_index = sub_df['cat_index'].values\n",
    "    temp = sub_df['temperature'].values.reshape(-1, 1)\n",
    "    \n",
    "    # convert cat indices to vectors\n",
    "    hots = []\n",
    "    for cat_ix in cat_index:\n",
    "        cat_ix = int(cat_ix)\n",
    "        vec = np.zeros(8)\n",
    "        vec[cat_ix]+=1.\n",
    "        hots.append(vec)\n",
    "    hots = np.array(hots)\n",
    "\n",
    "    return np.concatenate((hots, temp, time, conc_cat), axis=1)\n",
    "\n",
    "\n",
    "def select_at_least_one_cat(pt_data, num_points):\n",
    "    ''' make sure selection for source task has at least one of\n",
    "    all 8 types of ligand/catalyst. If not, this will spell\n",
    "    problems for the normalization of the parameters within the\n",
    "    meta planners.\n",
    "    '''\n",
    "    is_sat = False\n",
    "    while not is_sat:\n",
    "        # select random subset of pt_data\n",
    "        indices = np.arange(pt_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        ind = indices[:num_points]\n",
    "        sub_df = pt_data.iloc[ind, :]\n",
    "        if len(list(set(sub_df['cat_index']))) == 8:\n",
    "            is_sat = True\n",
    "    return sub_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE IX : 0\tRANDOM STATE : 448050\n",
      "SAMPLE IX : 1\tRANDOM STATE : 9767434\n",
      "SAMPLE IX : 2\tRANDOM STATE : 5875118\n",
      "SAMPLE IX : 3\tRANDOM STATE : 8411757\n",
      "SAMPLE IX : 4\tRANDOM STATE : 7962162\n",
      "SAMPLE IX : 5\tRANDOM STATE : 9430438\n",
      "SAMPLE IX : 6\tRANDOM STATE : 9223057\n",
      "SAMPLE IX : 7\tRANDOM STATE : 7335278\n",
      "SAMPLE IX : 8\tRANDOM STATE : 5983079\n",
      "SAMPLE IX : 9\tRANDOM STATE : 8593508\n",
      "SAMPLE IX : 10\tRANDOM STATE : 4373633\n",
      "SAMPLE IX : 11\tRANDOM STATE : 8100441\n",
      "SAMPLE IX : 12\tRANDOM STATE : 6541466\n",
      "SAMPLE IX : 13\tRANDOM STATE : 1027501\n",
      "SAMPLE IX : 14\tRANDOM STATE : 1194260\n",
      "SAMPLE IX : 15\tRANDOM STATE : 149048\n",
      "SAMPLE IX : 16\tRANDOM STATE : 6868645\n",
      "SAMPLE IX : 17\tRANDOM STATE : 4371917\n",
      "SAMPLE IX : 18\tRANDOM STATE : 1905597\n",
      "SAMPLE IX : 19\tRANDOM STATE : 4294464\n",
      "SAMPLE IX : 20\tRANDOM STATE : 4352916\n",
      "SAMPLE IX : 21\tRANDOM STATE : 7836205\n",
      "SAMPLE IX : 22\tRANDOM STATE : 6142514\n",
      "SAMPLE IX : 23\tRANDOM STATE : 879331\n",
      "SAMPLE IX : 24\tRANDOM STATE : 4179944\n",
      "SAMPLE IX : 25\tRANDOM STATE : 4258427\n",
      "SAMPLE IX : 26\tRANDOM STATE : 4010414\n",
      "SAMPLE IX : 27\tRANDOM STATE : 8513968\n",
      "SAMPLE IX : 28\tRANDOM STATE : 657425\n",
      "SAMPLE IX : 29\tRANDOM STATE : 1050703\n",
      "SAMPLE IX : 30\tRANDOM STATE : 7560156\n",
      "SAMPLE IX : 31\tRANDOM STATE : 75284\n",
      "SAMPLE IX : 32\tRANDOM STATE : 6127649\n",
      "SAMPLE IX : 33\tRANDOM STATE : 5782819\n",
      "SAMPLE IX : 34\tRANDOM STATE : 2831253\n",
      "SAMPLE IX : 35\tRANDOM STATE : 7688788\n",
      "SAMPLE IX : 36\tRANDOM STATE : 2942268\n",
      "SAMPLE IX : 37\tRANDOM STATE : 2570144\n",
      "SAMPLE IX : 38\tRANDOM STATE : 9371119\n",
      "SAMPLE IX : 39\tRANDOM STATE : 5616672\n"
     ]
    }
   ],
   "source": [
    "# generate test data for meta-learning planners for each case\n",
    "NUM_POINTS = 300\n",
    "NUM_RETURN = 8   # number of points per sample per source task\n",
    "NUM_SAMPLES = 40 # number of source task samples\n",
    "\n",
    "\n",
    "all_task_samples = []\n",
    "\n",
    "for sample_ix in range(NUM_SAMPLES):\n",
    "\n",
    "    # generate new random state\n",
    "    RANDOM_STATE = np.random.randint(0, 10e6) #100700\n",
    "    print(f'SAMPLE IX : {sample_ix}\\tRANDOM STATE : {RANDOM_STATE}')\n",
    "    #-------\n",
    "    # case 1\n",
    "    #-------\n",
    "\n",
    "    exp_pt = MIT_case1(noise_level=1)\n",
    "    random_state = np.random.RandomState(RANDOM_STATE)\n",
    "    planner = LHS(exp_pt.domain, random_state=random_state)\n",
    "\n",
    "    conditions = planner.suggest_experiments(NUM_POINTS)\n",
    "\n",
    "    exp_pt.run_experiments(conditions)\n",
    "    pt_data_1 = exp_pt.data\n",
    "\n",
    "    pt_data_1 = select_at_least_one_cat(pt_data_1, NUM_RETURN)\n",
    "\n",
    "    params = convert_params(pt_data_1) # shape (# observations, 11)\n",
    "    values = pt_data_1['y'].values.reshape(-1, 1)\n",
    "\n",
    "    case1_task = {'params': params, 'values': values}\n",
    "\n",
    "#    print(pt_data_1.shape, params.shape, values.shape)\n",
    "\n",
    "    #-------\n",
    "    # case 2\n",
    "    #-------\n",
    "\n",
    "    exp_pt = MIT_case2(noise_level=1)\n",
    "    random_state = np.random.RandomState(RANDOM_STATE)\n",
    "    planner = LHS(exp_pt.domain, random_state=random_state)\n",
    "\n",
    "    conditions = planner.suggest_experiments(NUM_POINTS)\n",
    "\n",
    "    exp_pt.run_experiments(conditions)\n",
    "    pt_data_2 = exp_pt.data\n",
    "\n",
    "    pt_data_2 = select_at_least_one_cat(pt_data_2, NUM_RETURN)\n",
    "\n",
    "    params = convert_params(pt_data_2) # shape (# observations, 11)\n",
    "    values = pt_data_2['y'].values.reshape(-1, 1)\n",
    "\n",
    "    case2_task = {'params': params, 'values': values}\n",
    "\n",
    "#    print(pt_data_2.shape, params.shape, values.shape)\n",
    "\n",
    "\n",
    "    #-------\n",
    "    # case 3\n",
    "    #-------\n",
    "\n",
    "    exp_pt = MIT_case3(noise_level=1)\n",
    "    random_state = np.random.RandomState(RANDOM_STATE)\n",
    "    planner = LHS(exp_pt.domain, random_state=random_state)\n",
    "\n",
    "    conditions = planner.suggest_experiments(NUM_POINTS)\n",
    "\n",
    "    exp_pt.run_experiments(conditions)\n",
    "    pt_data_3 = exp_pt.data\n",
    "\n",
    "    pt_data_3 = select_at_least_one_cat(pt_data_3, NUM_RETURN)\n",
    "\n",
    "    params = convert_params(pt_data_3) # shape (# observations, 11)\n",
    "    values = pt_data_3['y'].values.reshape(-1, 1)\n",
    "\n",
    "    case3_task = {'params': params, 'values': values}\n",
    "\n",
    "#    print(pt_data_2.shape, params.shape, values.shape)\n",
    "\n",
    "\n",
    "    #-------\n",
    "    # case 4\n",
    "    #-------\n",
    "\n",
    "    exp_pt = MIT_case4(noise_level=1)\n",
    "    random_state = np.random.RandomState(RANDOM_STATE)\n",
    "    planner = LHS(exp_pt.domain, random_state=random_state)\n",
    "\n",
    "    conditions = planner.suggest_experiments(NUM_POINTS)\n",
    "\n",
    "    exp_pt.run_experiments(conditions)\n",
    "    pt_data_4 = exp_pt.data\n",
    "\n",
    "    pt_data_4 = select_at_least_one_cat(pt_data_4, NUM_RETURN)\n",
    "\n",
    "    params = convert_params(pt_data_4) # shape (# observations, 11)\n",
    "    values = pt_data_4['y'].values.reshape(-1, 1)\n",
    "\n",
    "    case4_task = {'params': params, 'values': values}\n",
    "\n",
    "#    print(pt_data_4.shape, params.shape, values.shape)\n",
    "\n",
    "\n",
    "    #-------\n",
    "    # case 5\n",
    "    #-------\n",
    "\n",
    "    exp_pt = MIT_case5(noise_level=1)\n",
    "    random_state = np.random.RandomState(RANDOM_STATE)\n",
    "    planner = LHS(exp_pt.domain, random_state=random_state)\n",
    "\n",
    "    conditions = planner.suggest_experiments(NUM_POINTS)\n",
    "\n",
    "    exp_pt.run_experiments(conditions)\n",
    "    pt_data_5 = exp_pt.data\n",
    "\n",
    "    pt_data_5 = select_at_least_one_cat(pt_data_5, NUM_RETURN)\n",
    "\n",
    "    params = convert_params(pt_data_5) # shape (# observations, 11)\n",
    "    values = pt_data_5['y'].values.reshape(-1, 1)\n",
    "\n",
    "    case5_task = {'params': params, 'values': values}\n",
    "\n",
    "#    print(pt_data_5.shape, params.shape, values.shape)\n",
    "\n",
    "    # add sample\n",
    "    all_task_samples.append([case1_task, case2_task, case3_task, case4_task, case5_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of the tasks to the disk\n",
    "pickle.dump(all_task_samples, open('tasks_8.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the correlation between tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr12 = pearsonr(\n",
    "    case1_task['values'].ravel(), case2_task['values'].ravel()\n",
    ")[0]\n",
    "corr12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr13 = pearsonr(\n",
    "    case1_task['values'].ravel(), case3_task['values'].ravel()\n",
    ")[0]\n",
    "corr13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr14 = pearsonr(\n",
    "    case1_task['values'].ravel(), case4_task['values'].ravel()\n",
    ")[0]\n",
    "corr14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr15 = pearsonr(\n",
    "    case1_task['values'].ravel(), case5_task['values'].ravel()\n",
    ")[0]\n",
    "corr15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the locations of the optimia between tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_1 = np.argmax(case1_task['values'])\n",
    "max_yield_1 = case1_task['values'][argmax_1]\n",
    "opt_params_1 = case1_task['params'][argmax_1, :]\n",
    "\n",
    "opt_params_1, max_yield_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_2 = np.argmax(case2_task['values'])\n",
    "max_yield_2 = case2_task['values'][argmax_2]\n",
    "opt_params_2 = case2_task['params'][argmax_2, :]\n",
    "\n",
    "opt_params_2, max_yield_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_3 = np.argmax(case3_task['values'])\n",
    "max_yield_3 = case3_task['values'][argmax_3]\n",
    "opt_params_3 = case3_task['params'][argmax_3, :]\n",
    "\n",
    "opt_params_3, max_yield_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_4 = np.argmax(case4_task['values'])\n",
    "max_yield_4 = case4_task['values'][argmax_4]\n",
    "opt_params_4 = case4_task['params'][argmax_4, :]\n",
    "\n",
    "opt_params_4, max_yield_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_5 = np.argmax(case5_task['values'])\n",
    "max_yield_5 = case5_task['values'][argmax_5]\n",
    "opt_params_5 = case5_task['params'][argmax_5, :]\n",
    "\n",
    "opt_params_5, max_yield_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_1.sort_values(by='y', ascending=False).iloc[:15, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_2.sort_values(by='y', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_3.sort_values(by='y', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_4.sort_values(by='y', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_5.sort_values(by='y', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrage the columns to be in same order as optimization\n",
    "cols = ['cat_index', 'temperature', 't', 'conc_cat', 'y', 'computation_t', 'experiment_t', 'strategy']\n",
    "\n",
    "pt_data_1 = pt_data_1[cols]\n",
    "pt_data_2 = pt_data_2[cols]\n",
    "pt_data_3 = pt_data_3[cols]\n",
    "pt_data_4 = pt_data_4[cols]\n",
    "pt_data_5 = pt_data_5[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_1_hot = convert_params(pt_data_1)\n",
    "pt_data_1_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params_1 = convert_params(pt_data_1) #pt_data_1.iloc[:, :4].values\n",
    "train_values_1 = pt_data_1.iloc[:, 4].values.reshape(-1, 1)\n",
    "\n",
    "train_params_2 = convert_params(pt_data_2) #pt_data_2.iloc[:, :4].values\n",
    "train_values_2 = pt_data_2.iloc[:, 4].values.reshape(-1, 1)\n",
    "\n",
    "train_params_3 = convert_params(pt_data_3) #pt_data_3.iloc[:, :4].values\n",
    "train_values_3 = pt_data_3.iloc[:, 4].values.reshape(-1, 1)\n",
    "\n",
    "train_params_4 = convert_params(pt_data_4) #pt_data_4.iloc[:, :4].values\n",
    "train_values_4 = pt_data_4.iloc[:, 4].values.reshape(-1, 1)\n",
    "\n",
    "train_params_5 = convert_params(pt_data_5) #pt_data_5.iloc[:, :4].values\n",
    "train_values_5 = pt_data_5.iloc[:, 4].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    {'params': train_params_1, 'values': train_values_1},\n",
    "    {'params': train_params_2, 'values': train_values_2},\n",
    "    {'params': train_params_3, 'values': train_values_3},\n",
    "    {'params': train_params_4, 'values': train_values_4},\n",
    "    {'params': train_params_5, 'values': train_values_5},\n",
    "    \n",
    "]\n",
    "pickle.dump(tasks, open('tasks_10.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_processes.nps import NeuralProcess\n",
    "from neural_processes.observation_processor import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params_1 = normalizer.scalarize(train_params_1)\n",
    "train_values_1 = normalizer.scalarize(train_values_1)\n",
    "\n",
    "train_params_2 = normalizer.scalarize(train_params_2)\n",
    "train_values_2 = normalizer.scalarize(train_values_2)\n",
    "\n",
    "train_params_3 = normalizer.scalarize(train_params_3)\n",
    "train_values_3 = normalizer.scalarize(train_values_3)\n",
    "\n",
    "train_params_4 = normalizer.scalarize(train_params_4)\n",
    "train_values_4 = normalizer.scalarize(train_values_4)\n",
    "\n",
    "train_params_5 = normalizer.scalarize(train_params_5)\n",
    "train_values_5 = normalizer.scalarize(train_values_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {'params': train_params_1, 'values': train_values_1},\n",
    "    {'params': train_params_2, 'values': train_values_2},\n",
    "    {'params': train_params_3, 'values': train_values_3},\n",
    "    {'params': train_params_4, 'values': train_values_4},\n",
    "    {'params': train_params_5, 'values': train_values_5},\n",
    "    \n",
    "]\n",
    "\n",
    "tasks[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'model': {'learning_rate': 8e-4, 'epochs': 10000}\n",
    "}\n",
    "\n",
    "model = NeuralProcess(\n",
    "            x_dim=4,\n",
    "            y_dim=1, \n",
    "            use_self_attention=False, \n",
    "            use_cross_attention=True,\n",
    "            hyperparams=hyperparams,\n",
    ")\n",
    "\n",
    "model.train(tasks[1:], tasks[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the multi-task BayesOpt experiments with Summit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summit.strategies import MTBO, STBO, Transform, LHS, Chimera\n",
    "from summit.utils.dataset import DataSet\n",
    "from summit.domain import *\n",
    "import summit\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stbo(exp, max_iterations=10, categorical_method=\"one-hot\"):\n",
    "    exp.reset()\n",
    "    strategy = STBO(exp.domain, \n",
    "                    categorical_method=categorical_method)\n",
    "    r = summit.Runner(strategy=strategy, \n",
    "                      experiment=exp, \n",
    "                      max_iterations=max_iterations)\n",
    "    r.run()\n",
    "    return r\n",
    "\n",
    "def run_mtbo(exp, pt_data, max_iterations=10):\n",
    "    strategy = MTBO(exp.domain, \n",
    "                    pretraining_data=pt_data,\n",
    "                    categorical_method=\"one-hot\", \n",
    "                    task=1)\n",
    "    r = summit.Runner(strategy=strategy,\n",
    "                      experiment=exp, \n",
    "                      max_iterations=max_iterations)\n",
    "    r.run()\n",
    "    return r\n",
    "\n",
    "def make_average_plot(results: List[summit.Runner], ax, label=None, color=None):\n",
    "    objective = results[0].experiment.domain.output_variables[0].name\n",
    "    yields = [r.experiment.data[objective] for r in results]\n",
    "    yields = np.array(yields)\n",
    "    mean_yield = np.mean(yields, axis=0)\n",
    "    std_yield = np.std(yields, axis=0)\n",
    "    x = np.arange(0, len(mean_yield), 1).astype(int)\n",
    "    ax.plot(x, mean_yield, label=label, linewidth=2)\n",
    "    ax.fill_between(x, mean_yield-std_yield, mean_yield+std_yield, alpha=0.1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def make_comparison_plot(*args):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    for arg in args:\n",
    "        make_average_plot(arg['results'], ax, label=arg[\"label\"], color=arg.get(\"color\"))\n",
    "    fontdict = fontdict={\"size\":12}\n",
    "    ax.legend(loc = \"lower right\", prop=fontdict)\n",
    "    ax.set_xlim(0,20)\n",
    "    ax.set_xticks(np.arange(0, 20, 2).astype(int))\n",
    "    ax.set_ylabel('Yield', fontdict=fontdict)\n",
    "    ax.set_xlabel('Reactions', fontdict=fontdict)\n",
    "    ax.tick_params(direction='in')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REPEATS = 10\n",
    "MAX_ITER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task BayesOpt\n",
    "stbo_results = []\n",
    "for i in range(NUM_REPEATS):\n",
    "    print(f'STBO repeat number : {i+1}')\n",
    "    exp = MIT_case1(noise_level=1)\n",
    "    result = run_stbo(exp, max_iterations=MAX_ITER)\n",
    "    stbo_results.append(result)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the meta-data --> \n",
    "\n",
    "pt_data_2[('task', 'METADATA')] = np.zeros(pt_data_2.shape[0], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multi-task BayesOpt - one auxillary task (MIT case 2)\n",
    "mtbo_results = []\n",
    "for i in range(NUM_REPEATS):\n",
    "    print(f'MTBO repeat number : {i+1}')\n",
    "    exp = MIT_case1(noise_level=1)\n",
    "    result = run_mtbo(exp, pt_data_2, max_iterations=MAX_ITER)\n",
    "    mtbo_results.append(result)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-task BayesOpt with 8 points from all the tasks\n",
    "\n",
    "# prepare all the data (#2 already prepared in the correct format)\n",
    "pt_data_3[('task', 'METADATA')] = np.zeros(pt_data_3.shape[0], dtype=int)+1\n",
    "pt_data_4[('task', 'METADATA')] = np.zeros(pt_data_4.shape[0], dtype=int)+2\n",
    "pt_data_5[('task', 'METADATA')] = np.zeros(pt_data_5.shape[0], dtype=int)+3\n",
    "\n",
    "pt_data_all_meta = pd.concat([pt_data_2, pt_data_3, pt_data_4, pt_data_5])\n",
    "pt_data_all_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-task BayesOpt - one auxillary task (MIT case 2)\n",
    "mtbo_all_results = []\n",
    "for i in range(NUM_REPEATS):\n",
    "    print(f'MTBO repeat number : {i+1}')\n",
    "    exp = MIT_case1(noise_level=1)\n",
    "    result = run_mtbo(exp, pt_data_all_meta, max_iterations=MAX_ITER)\n",
    "    mtbo_all_results.append(result)\n",
    "    #clear_output(wait=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbo_dfs = []\n",
    "for run_ix, r in enumerate(mtbo_results):\n",
    "    objective = r.experiment.domain.output_variables[0].name\n",
    "    yields = r.experiment.data[objective].tolist()\n",
    "    dict_ = {'iter': np.arange(21)+1, 'yield': yields}\n",
    "    df = pd.DataFrame(dict_)\n",
    "    df['run_ix'] = run_ix\n",
    "    df['cummax_yield'] = df['yield'].cummax()\n",
    "    df['regret'] = 1.0 - df['cummax_yield'] \n",
    "    mtbo_dfs.append(df)\n",
    "    \n",
    "mtbo_df = pd.concat(mtbo_dfs)\n",
    "\n",
    "mtbo_all_dfs = []\n",
    "for run_ix, r in enumerate(mtbo_all_results):\n",
    "    objective = r.experiment.domain.output_variables[0].name\n",
    "    yields = r.experiment.data[objective].tolist()\n",
    "    dict_ = {'iter': np.arange(21)+1, 'yield': yields}\n",
    "    df = pd.DataFrame(dict_)\n",
    "    df['run_ix'] = run_ix\n",
    "    df['cummax_yield'] = df['yield'].cummax()\n",
    "    df['regret'] = 1.0 - df['cummax_yield'] \n",
    "    mtbo_all_dfs.append(df)\n",
    "    \n",
    "mtbo_all_df = pd.concat(mtbo_all_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison plots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.lineplot(data=mtbo_df, x='iter', y='cummax_yield')\n",
    "sns.lineplot(data=mtbo_all_df, x='iter', y='cummax_yield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison plots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.lineplot(data=mtbo_df, x='iter', y='regret')\n",
    "sns.lineplot(data=mtbo_all_df, x='iter', y='regret')\n",
    "\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
